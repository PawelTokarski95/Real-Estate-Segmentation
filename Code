
#Necessary libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import davies_bouldin_score
import scipy as stats
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, classification_report
from sklearn.metrics import silhouette_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score


#PART 1: EDA AND CLUSTERING

#I import my dataset

housing = pd.read_csv("C:/Users/pawel/Desktop/Data Science/housing.csv", sep=';', encoding='latin1')
#First 5 observations

housing.head()

#Initial statistical analysis of dataset

housing.describe()

#I encode some of the variables in order for them to fit to numerical task
le = LabelEncoder()
encoded = []
ohe = OneHotEncoder()
columns = ['mainroad','guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea', 'furnishing']
for i in range(0,5):
    encoded.append(le.fit_transform(housing.iloc[:,i+5]))


encoded.append(le.fit_transform(housing['prefarea']))
encoded.append(le.fit_transform(housing['furnishing']))


encoded_all = pd.DataFrame({columns[0]: encoded[0], columns[1]: encoded[1], columns[2]: encoded[2], columns[3]: encoded[3], columns[4]: encoded[4], columns[5]: encoded[5], columns[6]: encoded[6]})
for i in columns:
    housing = housing.drop(i, axis=1)

housingv2 = pd.concat([housing, encoded_all], axis=1)

housingv2

housingv2['price'].describe()


# Fitting by using stadard scaller and normalizing observations.
scaler = StandardScaler()

# Scalling columns
scaled_data = scaler.fit_transform(housingv2)

columns = housingv2.columns

scaled_df = pd.DataFrame(scaled_data, columns=columns)

#Correlations map
sns.heatmap(scaled_df.corr())

scaled_df.info()
scaled_df_cleaned = scaled_df.dropna(axis=0)
scaled_df_cleaned


DB = []

#Davis-Bouldin Method is a score by which an analyst may see the differences between the segments (clusters). The lower value - gradually better our
##fitted clusters are, due to clear distinction between the features.
for i in range(2,20):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(scaled_df)
    DB.append(davies_bouldin_score(scaled_df, kmeans.labels_))


plt.plot(range(2, 20), DB, marker='o')
plt.xlabel('Clusters number (k)')
plt.ylabel('Davies-Bouldin Score')
plt.title('Davies-Bouldin Method')
plt.show()


#Silhouette method on the other hand - measures how directly fitted are our features to the clusters. The higher score means better results.

SScore = []

for i in range(2,20):
    kmeans = KMeans(n_clusters=i, random_state=42)
    kmeans.fit(scaled_df)
    SScore.append(silhouette_score(scaled_df, kmeans.labels_))



plt.plot(range(2, 20), SScore, marker='o')
plt.xlabel('Clusters number (k)')
plt.ylabel('Silhouette_score')
plt.title('Silhouette Method')
plt.show()

#optimal value of cluters - 5
#the number of clusters is based upon the compromise between both scores: Silhouette score and Davies-Bouldin score

kmeansADJ = KMeans(n_clusters=5, random_state=42)
kmeansADJ.fit(scaled_data)



centroids = kmeansADJ.labels_



#PART 2: MODELING

X = scaled_df #Features
y = centroids #Clusters


#training and testing data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)


# Modelling

model = DecisionTreeClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print(classification_report(y_test, y_pred))

model2 = SVC(kernel='rbf', decision_function_shape='ovr', random_state=42)
model2.fit(X_train, y_train)
y_pred = model2.predict(X_test)

print(classification_report(y_test, y_pred))

model3 = RandomForestClassifier()

model3.fit(X_train, y_train)
y_pred = model3.predict(X_test)
print(classification_report(y_test, y_pred))


#Cross validation processes for making sure that models scores are reliable


cv1 = cross_val_score(model, X, y, cv=8,scoring='accuracy')
np.mean(cv1)

cv2 = cross_val_score(model2, X, y, cv=8,scoring='accuracy')
np.mean(cv2)

cv3 = cross_val_score(model3, X, y, cv=8,scoring='accuracy')
np.mean(cv3)























